import torch.nn.functional as F
import constants as c
import numpy as np
import torch
import sys

##------------------------------------------replayMemory class------------------------------------------------##
## GENERAL - This class is used to initiate a replay memory which can be used by the agent to update the neural
##           network it makes use of. The class facilitates the storage and retrieval of experiences that are
##           generated by the agent. Also the agent can call a learn_from_memory method which facilitates the
##           update of the neural networks given to the method. For this a policy and target network are required.
##           Experience replay architecture based on this video: https://www.youtube.com/watch?v=wc-FxNENg9U
##
## Variables:
##  Memories in general - The memories are initiated to store the different experiences. These are seperated by
##                        different numpy arrays, such that converting to pytorch tensors and indexing is easier.
##
##  next_state_memory, state_memory - In these np.arrays the observed next state and current state is stored. The
##                                    The size is such that one observation of the state fits. It is a nested numpy
##                                    array in which every nested array is equal to the size of the state.
##
##  action_memory - In this numpy array the action is stored that is taken by the agent. The action is linked
##                  to the state in the state_memory, hence forming a state-action pair.
##
##  reward_memory - Numpy array containing the reward for ending up in the next_state. The reward is one simple integer
##                  returned by the gym environment.
##
## terminal_memory - Numpy array containing a boolean representation if a next_state is a terminal state, a state
##                   in which the episode is terminated. This array is needed to prevent the agent from learning
##                   from terminal states. This array is used to set q-values of a terminal state to 0, hence
##                   stating that the q-value for that state_action pair is useless.
##
## amount_of_experiences_pushed - Variable to keep track how many experiences are pushed. This is used to index where
##                                an experience should be stored in a memory array. Also this number is compared to the
##                                the memory capacity to replace old experiences when the memory is full.
##
## Functions:
##
## store_experience() - This function provides the storage of the experience when called. It stores all values needed
##                      for a DQN neural network to learn. It stores the values in np.arrays on a given index. The
##                      index is determined by the amount experiences that are already in the replay memory. If the
##                      amount of experiences exceeds the memory capacity the function will begin again at the first
##                      position of the memory array. In this it replaces the oldest memories. This ensures better
##                      learning by the agent, since old memories might be based on old unuseful policies.
##
## can_memory_provide_sample_of_batch_size() - This function checks whether there are enough experiences gathered to
##                                             make a sample that can be used to update the neural network. This is
##                                             because a neural network does not learn from 1 experience but from
##                                             multiple experiences.It compares the amount of experiences to the
##                                             batch size provided by the constants file.
##
##
## convert_experience_sample_to_tensor() - Pytorch only accepts tensors in a forward pass. In order to learn from the
##                                         samples the samples that are used in a forward/backward pass should be of
##                                         type tensor. Note, action and terminal sample are not converted to tensors.
##                                         This is because these are only used to index the sample tensors. For this
##                                         usage they can be of type np.array.
##
## sample_experience_from_replay_memory() - Takes a sample from the replay memory and returns the variables needed for
##                                          neural network to learn and updates its weights.
##                                          It checks whether there are less or more experiences stored than memory capacity.
##                                          If there are less experiences stored than the memory capacity, the function
##                                          only takes a sample of the experiences that are actually stored. In this
##                                          way the function doesn't sample from empty experience arrays. If the amount
##                                          of experiences stored exceeds the MEMORY_CAPACITY, then the function samples
##                                          from the full memory buffer. Note, that when the amount of experience stored
##                                          exceeds the memory capacity, the exceeding experiences are stored at the beginning
##                                          of the replay buffer, replacing the oldest experiences. This is handled in the
##                                          function store_experience()
##
## update_networks_using_replay_memory() - Using the samples from the replay memory this function calculates the loss
##                                         of the policy network compared to the maximum Q-values of the next state.
##                                         To stabilize the calculation the function makes use of a target network, this
##                                         avoids the Q-network from chasing itself. After caluclating the loss by using
##                                         the Q-values from the experiences of the experience replay, the function does
##                                         a backward step to optimize the network. This is done by internal PyTorch methods
##
##
class replayMemoryDQN():
    def __init__(self):

        #Initiate the memories used for the replay memory.
        self.next_state_memory = np.zeros((c.MEMORY_CAPACITY, c.OBSERVATION_SPACE), dtype=np.float32)
        self.state_memory      = np.zeros((c.MEMORY_CAPACITY, c.OBSERVATION_SPACE), dtype=np.float32)
        self.action_memory     = np.zeros(c.MEMORY_CAPACITY, dtype=np.float32) #Note to self: what if there are multiple actions?
        self.reward_memory     = np.zeros(c.MEMORY_CAPACITY, dtype=np.int32)
        self.terminal_memory   = np.zeros(c.MEMORY_CAPACITY, dtype=np.bool)
        self.amount_of_experiences_in_memory = 0

        return

    def store_experience(self, state, new_state, reward, action, done):

        index_to_store_experience = self.amount_of_experiences_in_memory % c.MEMORY_CAPACITY
        try:
            self.next_state_memory[index_to_store_experience] = new_state
            self.terminal_memory[index_to_store_experience]   = done
            self.reward_memory[index_to_store_experience]     = reward
            self.action_memory[index_to_store_experience]     = action
            self.state_memory[index_to_store_experience]      = state
        except IndexError:
            print("Something wrong with indexing the memory, the programm cannot store on this index: {}".format(index_to_store_experience))
            print("Maximum index is {}".format(c.MEMORY_CAPACITY))
            print("Program terminated")
            sys.exit(1)
        except Exception as e:
            print("Unknown error has occured during storing the program:")
            print(e.message)
            print(e.args)
            print("Program terminated")
            sys.exit(1)

        self.amount_of_experiences_in_memory += 1

        return

    def can_memory_provide_sample_of_batch_size(self):
        return self.amount_of_experiences_in_memory >= c.BATCH_SIZE

    def convert_experience_sample_to_tensor(self, sample_new_states, sample_rewards, sample_states):

        try:
            new_state_tensor        = torch.tensor(sample_new_states).to(c.DEVICE)
            reward_sample_tensor    = torch.tensor(sample_rewards).to(c.DEVICE)
            state_sample_tensor     = torch.tensor(sample_states).to(c.DEVICE)
        except:
            print("Something went wrong during the conversion of the samples to tensors.")
            print("Might there be a problem with parsing the variable into torch.tensor()?")
            print("New_state_sample:{}".format(sample_new_states))
            print("sample_reward:{}".format(sample_rewards))
            print("sample_states:{}".format(sample_states))
            print("Type of sample_new_states: {}".format(type(sample_new_states)))
            print("Type of sample_rewards: {}".format(type(sample_rewards)))
            print("Type of sample_states: {}".format(type(sample_states)))
            print("Program terminated!")
            sys.exit(1)

        return new_state_tensor, reward_sample_tensor, state_sample_tensor

    def sample_experience_from_replay_memory(self):

        # Check how many experiences are in the replay buffer. If amount exceeds the memory capacity then sample from
        # The full range of the mem_capacity. Else only sample from the experiences that are actually there.

        amount_experiences_in_replay_buffer = min(self.amount_of_experiences_in_memory, c.MEMORY_CAPACITY)

        #Select a BATCH_size amount of random indices to sample from the replay memory
        #Replace is set to false, assuring no double samples are taken and learned from
        random_sample_indeces = np.random.choice(amount_experiences_in_replay_buffer, c.BATCH_SIZE, replace=False)

        #Sample the random experience from the memories according to the rendom indeces
        try:
            new_state_sample = self.next_state_memory[random_sample_indeces]
            terminal_sample  = self.terminal_memory[random_sample_indeces]
            action_sample    = self.action_memory[random_sample_indeces]
            reward_sample    = self.reward_memory[random_sample_indeces]
            state_sample     = self.state_memory[random_sample_indeces]
        except IndexError:
            print("Something went wrong while indexing the memory with the random sample indices")
            print("random_sample_indeces: {}".format(random_sample_indeces))
            print("Type random_sample_indeces: {}".format(type(random_sample_indeces)))
            print("Program terminated!")
            sys.exit(1)
        except:
            print("An unknown error has occurred during indexing the memory with random sample indices")
            print("Program terminated!")
            sys.exit(1)

        new_state_sample_tensor, reward_sample_tensor, state_sample_tensor = \
            self.convert_experience_sample_to_tensor(new_state_sample, reward_sample, state_sample)

        return new_state_sample_tensor, reward_sample_tensor, state_sample_tensor, terminal_sample, action_sample

    def update_networks_using_replay_memory(self, policy_network, target_network):

        #If there are not yet enough samples in the experience memory, the program won't learn but continues exploring
        if self.can_memory_provide_sample_of_batch_size() == False:
            print("Network cannot yet learn from the experience replay, there are not enough experiences in the replay memory!")
            print("Program skipped the learning and continues exploring.")
            print("The program will learn when enough experiences are in the experience replay")
            return

        new_state_sample_batch, reward_sample_batch, state_sample_batch, terminal_sample, action_sample = \
            self.sample_experience_from_replay_memory()

        #Make a numpy array ranging from 0 to BATCH_SIZE, used for indexing the sample tensors:
        #Numpy array looks like this: [0,1,2,3,...,BATCH_SIZE] -> 0 up to BATCH_SIZE
        batch_indexer = np.arange(c.BATCH_SIZE, dtype=np.int32)

        #Retrieve variables to compute loss

        #Forward pass returns q-values, for every sample in the batch the Q-value of the action is selected
        #This results in a 1 dimensional tensor consisting of Q-values of the action taken in the samples state:
        #Tensor looks like: torch.tensor([q_value1, q_value2,...,q_valueBATCHINDEX])

        Q_values_state_policy_network = policy_network.forward(state_sample_batch)[batch_indexer, action_sample]

        #Retrieve the Q-values for every possible action in the state the agant landed in after taking the action
        #according to the action_sample in the state according to the state_sample_batch
        #This results in a tensor of size (c.BATCH_SIZE, c.ACTION_SPACE)
        #Tensor looks like: torch.tensor([[Q_value_action1, Q_value_action2, ... , Q_value_actionACTION_SPACE],
        #                                  [Q_value_action1, Q_value_action2, ..., Q_value_actionACTION_SPACE],
        #                                  ...,
        #      ->Up to c.BATCH_SIZE        [Q_value_action1, Q_value_action2, ... , Q_value_actionACTION_SPACE]])

        Q_values_next_state_target   = target_network.forward(new_state_sample_batch)

        # Zero the Q-values of zero, since the future expected reward is 0
        Q_values_next_state_target[terminal_sample] = 0.0

        max_Q_values_next_state_target = torch.max(Q_values_next_state_target, dim=1)[0].to(c.DEVICE)
        Q_values_to_update_to = reward_sample_batch + (c.GAMMA * max_Q_values_next_state_target)

        lossFunction = F.mse_loss(Q_values_to_update_to, Q_values_state_policy_network).to(c.DEVICE)
        policy_network.optimizer.zero_grad()
        lossFunction.backward()
        policy_network.optimizer.step()

        return